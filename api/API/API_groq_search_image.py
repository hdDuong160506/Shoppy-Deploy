import os
import requests
import base64
import re
from dotenv import load_dotenv
from supabase import create_client, Client
from difflib import SequenceMatcher

load_dotenv()

# Groq Llama 4 Scout Vision - MODEL M·ªöI NH·∫§T 2025
GROQ_SEARCH_IMAGE_API_KEY = os.getenv("GROQ_SEARCH_IMAGE_API_KEY")
VISION_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"  # Llama 4 Scout - Vision model m·ªõi nh·∫•t
# C√°c model kh√°c:
# - "llama-3.2-90b-vision-preview" (Vision 3.2 - 90B)
# - "llama-3.2-11b-vision-preview" (Vision 3.2 - 11B)

# Supabase
DATA_BASE_SECRET_KEY_SUPABASE = os.getenv("DATA_BASE_SECRET_KEY_SUPABASE")
DATA_BASE_URL_SUPABASE = os.getenv("DATA_BASE_URL_SUPABASE")

url = DATA_BASE_URL_SUPABASE
key = DATA_BASE_SECRET_KEY_SUPABASE
supabase: Client = create_client(url, key)


# ==================== HELPER FUNCTIONS ====================

def fetch_product_names():
    """L·∫•y danh s√°ch t√™n product t·ª´ Supabase"""
    try:
        response = supabase.table("product").select("name").execute()
        rows = response.data
        if not rows:
            print("‚ö†Ô∏è D·ªØ li·ªáu r·ªóng t·ª´ Supabase")
            return []

        names = {row["name"].strip() for row in rows if row.get("name")}
        return list(names)

    except Exception as e:
        print(f"‚ö†Ô∏è Exception fetch_product_names: {e}")
        return []


def normalize_text(text: str) -> str:
    """Chu·∫©n h√≥a text ƒë·ªÉ so s√°nh"""
    text = text.lower().strip()
    text = re.sub(r'[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', '', text)
    return text


def fuzzy_match_product(detected_text: str, products: list) -> str:
    """
    So s√°nh m·ªù ƒë·ªÉ t√¨m s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t v·ªõi multi-level matching
    """
    detected_normalized = normalize_text(detected_text)
    
    print(f"üîç ƒêang t√¨m ki·∫øm cho: '{detected_normalized}'")
    
    best_match = None
    best_score = 0.0
    
    for product in products:
        product_normalized = normalize_text(product)
        
        # Level 1: Exact match (priority cao nh·∫•t)
        if detected_normalized == product_normalized:
            print(f"  ‚úì‚úì‚úì Exact match: '{product}'")
            return product
        
        # Level 2: Substring match
        if detected_normalized in product_normalized:
            score = 0.95
            print(f"  ‚úì‚úì Substring match (detected in product): '{product}' (score: {score})")
            if score > best_score:
                best_score = score
                best_match = product
        elif product_normalized in detected_normalized:
            score = 0.90
            print(f"  ‚úì‚úì Substring match (product in detected): '{product}' (score: {score})")
            if score > best_score:
                best_score = score
                best_match = product
        
        # Level 3: Word overlap match
        detected_words = set(detected_normalized.split())
        product_words = set(product_normalized.split())
        
        if detected_words & product_words:
            common_words = detected_words & product_words
            common_ratio = len(common_words) / max(len(detected_words), len(product_words))
            
            if common_ratio > 0.5 and common_ratio > best_score:
                best_score = common_ratio
                best_match = product
                print(f"  ‚úì Word match: '{product}' | Common words: {common_words} (score: {common_ratio:.2f})")
        
        # Level 4: Fuzzy similarity
        similarity = SequenceMatcher(None, detected_normalized, product_normalized).ratio()
        if similarity > 0.65 and similarity > best_score:
            best_score = similarity
            best_match = product
            print(f"  ‚úì Fuzzy match: '{product}' (score: {similarity:.2f})")
    
    if best_match:
        print(f"‚úÖ Best match found: '{best_match}' (confidence: {best_score:.2f})")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y match cho '{detected_text}'")
    
    return best_match


def prepare_image_data(image_data: str):
    """
    Chu·∫©n b·ªã image data cho Groq API (base64)
    Returns: (base64_string, mime_type) ho·∫∑c (None, None)
    """
    try:
        # N·∫øu l√† URL
        if image_data.startswith('http://') or image_data.startswith('https://'):
            print(f"üì• ƒêang t·∫£i ·∫£nh t·ª´ URL: {image_data[:50]}...")
            response = requests.get(image_data, timeout=15)
            if response.status_code == 200:
                base64_data = base64.b64encode(response.content).decode('utf-8')
                mime_type = response.headers.get('Content-Type', 'image/jpeg')
                print(f"‚úÖ ƒê√£ t·∫£i ·∫£nh th√†nh c√¥ng, MIME type: {mime_type}")
                return base64_data, mime_type
            else:
                print(f"‚ö†Ô∏è L·ªói t·∫£i ·∫£nh: HTTP {response.status_code}")
        
        # N·∫øu l√† base64 string v·ªõi data URL
        elif image_data.startswith('data:image'):
            match = re.match(r'data:([^;]+);base64,(.+)', image_data)
            if match:
                mime_type = match.group(1)
                base64_data = match.group(2)
                print(f"‚úÖ ƒê√£ parse data URL, MIME type: {mime_type}")
                return base64_data, mime_type
        
        # N·∫øu l√† raw base64 (kh√¥ng c√≥ prefix)
        else:
            print("‚úÖ S·ª≠ d·ª•ng raw base64 data")
            return image_data, "image/jpeg"
        
        return None, None
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói prepare_image_data: {str(e)}")
        return None, None


def safe_extract_text_from_groq_response(response_data: dict):
    """
    Tr√≠ch xu·∫•t text t·ª´ response Groq m·ªôt c√°ch an to√†n
    """
    try:
        if not response_data:
            return None
        
        # Ki·ªÉm tra error
        if "error" in response_data:
            error = response_data["error"]
            error_msg = error.get('message', 'Unknown error')
            error_type = error.get('type', 'unknown')
            print(f"‚ö†Ô∏è Groq API error [{error_type}]: {error_msg}")
            return None
        
        # L·∫•y content t·ª´ choices
        if "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            
            # Ki·ªÉm tra finish_reason
            finish_reason = choice.get("finish_reason")
            if finish_reason:
                print(f"‚ÑπÔ∏è Finish reason: {finish_reason}")
            
            if finish_reason and finish_reason not in ["stop", "length"]:
                print(f"‚ö†Ô∏è Unusual finish_reason: {finish_reason}")
            
            # L·∫•y text
            if "message" in choice and "content" in choice["message"]:
                text = choice["message"]["content"].strip()
                if text:
                    print(f"‚úÖ Extracted text: '{text}'")
                    return text
        
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y content trong response")
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error parsing Groq response: {e}")
        return None


def clean_detected_text(text: str) -> str:
    """
    L√†m s·∫°ch text t·ª´ AI response - version n√¢ng cao
    """
    if not text:
        return ""
    
    original_text = text
    
    # L√†m s·∫°ch k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = text.replace('"', '').replace('*', '').replace('`', '').replace('[', '').replace(']', '').strip()
    
    # X·ª≠ l√Ω c√°c format c√≥ th·ªÉ c√≥
    if ":" in text:
        text = text.split(":")[-1].strip()
    if "\n" in text:
        text = text.split("\n")[0].strip()
    
    # Lo·∫°i b·ªè c√°c t·ª´ th·ª´a (expanded list)
    stop_words = [
        "output", "result", "product", "m√≥n", "l√†", "is", "answer", ":", 
        "t√™n", "s·∫£n ph·∫©m", "ƒë√°p √°n", "the", "this is", "it is",
        "looks like", "appears to be", "seems to be", "probably"
    ]
    
    text_lower = text.lower()
    for word in stop_words:
        if text_lower.startswith(word):
            text = text[len(word):].strip()
            text_lower = text.lower()
    
    # Lo·∫°i b·ªè d·∫•u ch·∫•m c√¢u cu·ªëi
    text = text.rstrip('.,;:!?')
    
    if text != original_text:
        print(f"üßπ Cleaned: '{original_text}' ‚Üí '{text}'")
    
    return text


# ==================== MAIN FUNCTION ====================

def groq_search_product_by_image(image_data: str):
    """
    T√¨m s·∫£n ph·∫©m b·∫±ng h√¨nh ·∫£nh s·ª≠ d·ª•ng Groq Llama 4 Scout Vision API
    
    Args:
        image_data: URL ·∫£nh, base64 string, ho·∫∑c data URL
    
    Returns:
        str: T√™n s·∫£n ph·∫©m t√¨m ƒë∆∞·ª£c, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    print("\n" + "="*70)
    print("üöÄ GROQ LLAMA 4 SCOUT VISION - PRODUCT SEARCH")
    print("="*70)
    
    # B∆∞·ªõc 1: L·∫•y danh s√°ch s·∫£n ph·∫©m
    print("\nüì¶ [1/7] ƒêang l·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ Supabase...")
    products = fetch_product_names()
    
    if not products:
        print("‚ùå Danh s√°ch s·∫£n ph·∫©m r·ªóng")
        return None
    
    print(f"‚úÖ ƒê√£ load {len(products)} s·∫£n ph·∫©m")
    
    if not GROQ_SEARCH_IMAGE_API_KEY:
        print("‚ùå Thi·∫øu GROQ_SEARCH_IMAGE_API_KEY trong .env")
        return None
    
    # B∆∞·ªõc 2: Chu·∫©n b·ªã image data
    print("\nüñºÔ∏è [2/7] ƒêang x·ª≠ l√Ω image data...")
    base64_image, mime_type = prepare_image_data(image_data)
    
    if not base64_image:
        print("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω image data")
        return None
    
    # B∆∞·ªõc 3: T·∫°o prompt t·ªëi ∆∞u cho Llama 4 Scout
    print("\n‚úçÔ∏è [3/7] ƒêang t·∫°o prompt...")
    
    # Chia nh·ªè danh s√°ch n·∫øu qu√° d√†i (tr√°nh v∆∞·ª£t token limit)
    if len(products) > 100:
        print(f"‚ö†Ô∏è Danh s√°ch s·∫£n ph·∫©m l·ªõn ({len(products)} items), s·ª≠ d·ª•ng format t·ªëi ∆∞u")
        product_list = "\n".join([f"{i+1}. {p}" for i, p in enumerate(products[:100])])
        product_list += f"\n... and {len(products)-100} more items"
    else:
        product_list = "\n".join([f"‚Ä¢ {p}" for p in products])
    
        # PROMPT T·ªêI ∆ØU - X·ª¨ L√ù C·∫¢ ƒê·ªí ƒÇN V√Ä S·∫¢N PH·∫®M KH√ÅC
    prompt = f"""B·∫°n l√† chuy√™n gia nh·∫≠n di·ªán h√¨nh ·∫£nh v·ªõi kh·∫£ nƒÉng x√°c ƒë·ªãnh s·∫£n ph·∫©m ch√≠nh x√°c.

### DANH S√ÅCH S·∫¢N PH·∫®M TRONG C·ª¨A H√ÄNG (PH√ÇN LO·∫†I):
{product_list}

### QUY T·∫ÆC X·ª¨ L√ù ƒê·∫∂C BI·ªÜT QUAN TR·ªåNG:

1. **TR∆Ø·ªúNG H·ª¢P 1: NH·∫¨N DI·ªÜN R√ï R√ÄNG**
   - N·∫øu h√¨nh ·∫£nh r√µ r√†ng, nh·∫≠n di·ªán ƒë∆∞·ª£c s·∫£n ph·∫©m C·ª§ TH·ªÇ trong danh s√°ch
   - ‚Üí Tr·∫£ v·ªÅ t√™n s·∫£n ph·∫©m C·ª§ TH·ªÇ ƒë√≥

2. **TR∆Ø·ªúNG H·ª¢P 2: KH√ì PH√ÇN BI·ªÜT GI·ªÆA 2+ S·∫¢N PH·∫®M T√äN G·∫¶N GI·ªêNG**
   - N·∫øu h√¨nh ·∫£nh m√≥n ƒÉn kh√≥ ph√¢n bi·ªát gi·ªØa c√°c bi·∫øn th·ªÉ
   - HO·∫∂C nh·∫≠n di·ªán chung chung m·ªôt lo·∫°i s·∫£n ph·∫©m
   - ‚Üí Tr·∫£ v·ªÅ t√™n CHUNG/T·ªîNG QU√ÅT (ƒë·ªÉ match v·ªõi nhi·ªÅu s·∫£n ph·∫©m)

   **V√ç D·ª§:**
   - ·∫¢nh c∆°m kh√¥ng r√µ lo·∫°i ‚Üí "c∆°m" (match v·ªõi: c∆°m t·∫•m, c∆°m ch√°y, c∆°m g√†...)
   - ·∫¢nh b√∫n kh√¥ng r√µ lo·∫°i ‚Üí "b√∫n" (match v·ªõi: b√∫n b√≤, b√∫n ri√™u, b√∫n ch·∫£...)
   - ·∫¢nh ph·ªü kh√¥ng r√µ lo·∫°i ‚Üí "ph·ªü" (match v·ªõi: ph·ªü b√≤, ph·ªü g√†...)
   - ·∫¢nh √°o thun chung ‚Üí "√°o thun" (match v·ªõi: √°o thun nam, √°o thun n·ªØ...)

3. **TR∆Ø·ªúNG H·ª¢P 3: ·∫¢NH C√ì NHI·ªÄU S·∫¢N PH·∫®M**
   - N·∫øu ·∫£nh c√≥ nhi·ªÅu m√≥n/s·∫£n ph·∫©m kh√°c nhau
   - ‚Üí Tr·∫£ v·ªÅ danh s√°ch "s·∫£n ph·∫©m 1, s·∫£n ph·∫©m 2, ..." (t√™n chung)
   - Gi·ªõi h·∫°n t·ªëi ƒëa 3 s·∫£n ph·∫©m ch√≠nh

4. **TR∆Ø·ªúNG H·ª¢P 4: KH√îNG NH·∫¨N DI·ªÜN ƒê∆Ø·ª¢C**
   - N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªõi b·∫•t k·ª≥ s·∫£n ph·∫©m n√†o
   - ‚Üí Tr·∫£ v·ªÅ t·ª´ kh√≥a m√¥ t·∫£ chung nh·∫•t

### PH√ÇN T√çCH H√åNH ·∫¢NH THEO C·∫§P ƒê·ªò:

**C·∫§P 1: PH√ÇN LO·∫†I CH√çNH**
- ƒê·ªí ƒÇN/TH·ª®C U·ªêNG hay S·∫¢N PH·∫®M KH√ÅC?
- N·∫øu ƒë·ªì ƒÉn: C∆°m/B√∫n/Ph·ªü/M√¨/B√°nh/Th·ª©c u·ªëng?
- N·∫øu s·∫£n ph·∫©m kh√°c: VƒÉn ph√≤ng ph·∫©m/ƒê·ªì d√πng/Qu·∫ßn √°o?

**C·∫§P 2: PH√ÇN BI·ªÜT CHI TI·∫æT (CH·ªà KHI R√ï R√ÄNG)**
- V·ªõi ƒë·ªì ƒÉn: Lo·∫°i c·ª• th·ªÉ? C√≥ th·ªãt g√†/b√≤/heo?
- V·ªõi s·∫£n ph·∫©m: Ki·ªÉu d√°ng/m√†u s·∫Øc/ch·∫•t li·ªáu?

**C·∫§P 3: ƒê·ªò CH√çNH X√ÅC**
- R·∫•t r√µ (90-100%): Tr·∫£ t√™n c·ª• th·ªÉ
- T∆∞∆°ng ƒë·ªëi r√µ (70-90%): Tr·∫£ t√™n chung
- Kh√¥ng r√µ (<70%): Tr·∫£ t·ª´ kh√≥a chung

### QUY T·∫ÆC ∆ØU TI√äN CHO ƒê·ªí ƒÇN VI·ªÜT NAM:

**NH·∫¨N BI·∫æT LO·∫†I TH·ª∞C PH·∫®M:**
- H·∫†T C∆†M ‚Üí "c∆°m" (kh√¥ng ph·∫£i b√∫n/ph·ªü)
- S·ª¢I TR·∫ÆNG TR√íN ‚Üí "b√∫n"
- S·ª¢I TR·∫ÆNG D·∫∏T ‚Üí "ph·ªü"
- S·ª¢I V√ÄNG ‚Üí "m√¨"
- N∆Ø·ªöC D√ôNG ‚Üí x√°c ƒë·ªãnh lo·∫°i (trong/ƒë·ªè/ƒë·∫≠m)

**PH√ÇN BI·ªÜT M√ìN T∆Ø∆†NG T·ª∞:**
- C∆†M G√Ä vs B√öN G√Ä: C√≥ n∆∞·ªõc d√πng kh√¥ng? S·ª£i hay h·∫°t?
- B√öN B√í vs PH·ªû B√í: S·ª£i tr√≤n hay d·∫πt? N∆∞·ªõc d√πng m√†u g√¨?
- C∆†M T·∫§M vs C∆†M TH∆Ø·ªúNG: G·∫°o t·∫•m v·ª° hay nguy√™n h·∫°t?

### X·ª¨ L√ù T√äN G·∫¶N GI·ªêNG TRONG DATABASE:

N·∫øu database c√≥ c√°c s·∫£n ph·∫©m:
- "C∆°m t·∫•m s∆∞·ªùn"
- "C∆°m t·∫•m tr·ª©ng"
- "C∆°m t·∫•m ch·∫£"
- "C∆°m g√† x·ªëi m·ª°"
- "C∆°m b√≤ l√∫c l·∫Øc"

**KHI ·∫¢NH KH√îNG R√ï:**
- ·∫¢nh c∆°m chung ‚Üí "c∆°m" (match v·ªõi T·∫§T C·∫¢ m√≥n c∆°m)
- ·∫¢nh c∆°m t·∫•m kh√¥ng r√µ lo·∫°i ‚Üí "c∆°m t·∫•m" (match v·ªõi c∆°m t·∫•m s∆∞·ªùn/tr·ª©ng/ch·∫£)
- ·∫¢nh c∆°m c√≥ th·ªãt g√† ‚Üí "c∆°m g√†" (match v·ªõi c∆°m g√† x·ªëi m·ª°/...)

### ƒê·∫¶U RA L·ªäNH HO·∫†T THEO T√åNH HU·ªêNG:

**T√åNH HU·ªêNG 1: 1 s·∫£n ph·∫©m r√µ r√†ng**
‚Üí "T√™n s·∫£n ph·∫©m c·ª• th·ªÉ"

**T√åNH HU·ªêNG 2: 1 s·∫£n ph·∫©m kh√¥ng r√µ lo·∫°i**
‚Üí "T√™n chung" (vd: "c∆°m", "b√∫n", "√°o thun")

**T√åNH HU·ªêNG 3: Nhi·ªÅu s·∫£n ph·∫©m**
‚Üí "s·∫£n ph·∫©m 1, s·∫£n ph·∫©m 2, s·∫£n ph·∫©m 3" (t√™n chung)

**T√åNH HU·ªêNG 4: Kh√¥ng ch·∫Øc ch·∫Øn**
‚Üí "t·ª´ kh√≥a m√¥ t·∫£" (vd: "ƒë·ªì ƒÉn", "th·ª©c u·ªëng", "vƒÉn ph√≤ng ph·∫©m")

### CU·ªêI C√ôNG - QUY·∫æT ƒê·ªäNH:
1. Quan s√°t ·∫£nh k·ªπ
2. So s√°nh v·ªõi danh s√°ch s·∫£n ph·∫©m
3. √Åp d·ª•ng quy t·∫Øc x·ª≠ l√Ω ph√π h·ª£p
4. Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë√∫ng format

### ƒê·∫¶U RA B·∫ÆT BU·ªòC:
Ch·ªâ tr·∫£ v·ªÅ:
- T√™n s·∫£n ph·∫©m C·ª§ TH·ªÇ (n·∫øu r√µ)
- HO·∫∂C T√™n CHUNG (n·∫øu kh√¥ng r√µ)
- HO·∫∂C "s·∫£n ph·∫©m 1, s·∫£n ph·∫©m 2" (n·∫øu nhi·ªÅu s·∫£n ph·∫©m)
KH√îNG th√™m b·∫•t k·ª≥ ch·ªØ n√†o kh√°c, kh√¥ng gi·∫£i th√≠ch.
"""
    
    # B∆∞·ªõc 4: G·ªçi Groq Llama 4 Scout Vision API
    print(f"\nü§ñ [4/7] ƒêang g·ªçi Groq API v·ªõi model: {VISION_MODEL}...")
    
    api_url = "https://api.groq.com/openai/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {GROQ_SEARCH_IMAGE_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": VISION_MODEL,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "temperature": 0.05,  # R·∫•t th·∫•p ƒë·ªÉ c√≥ k·∫øt qu·∫£ nh·∫•t qu√°n
        "max_tokens": 200,    # ƒê·ªß cho t√™n s·∫£n ph·∫©m
        "top_p": 0.9,
        "stream": False
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        print(f"üì° Vision API Status: {response.status_code}")
        
        if response.status_code != 200:
            print(f"‚ùå API error {response.status_code}: {response.text[:200]}")
            return None
        
        res = response.json()
        
        # B∆∞·ªõc 5: Tr√≠ch xu·∫•t text
        print("\nüìù [5/7] ƒêang tr√≠ch xu·∫•t k·∫øt qu·∫£...")
        text = safe_extract_text_from_groq_response(res)
        
        if not text:
            print("‚ùå Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ response")
            return None
        
        # B∆∞·ªõc 6: L√†m s·∫°ch text
        print("\nüßπ [6/7] ƒêang l√†m s·∫°ch output...")
        text = clean_detected_text(text)
        print(f"üéØ Llama 4 Scout detected: '{text}'")
        
        # B∆∞·ªõc 7: Fuzzy matching
        print("\nüîç [7/7] ƒêang so kh·ªõp v·ªõi database...")
        matched_product = fuzzy_match_product(text, products)
        
        if matched_product:
            print("\n" + "="*70)
            print(f"‚úÖ SUCCESS! Found product: '{matched_product}'")
            print("="*70)
            return matched_product
        
        # B∆∞·ªõc 8: Fallback strategy - keyword matching
        print("\n‚ö†Ô∏è Fuzzy matching failed, trying fallback strategy...")
        
        keywords = [
            # ƒê·ªì ƒÉn
            "c∆°m", "ph·ªü", "b√∫n", "b√°nh", "ch·∫£", "g√†", "b√≤", "heo", "t√¥m", "c√°",
            "m√¨", "canh", "l·∫©u", "nem", "g·ªèi", "x√¥i", "ch√°o",
            # ƒê·ªì u·ªëng  
            "tr√†", "c√† ph√™", "n∆∞·ªõc", "sinh t·ªë", "s·ªØa", "bia", "r∆∞·ª£u", "chanh",
            # ƒê·ªì d√πng
            "b√∫t", "v·ªü", "s√°ch", "balo", "t√∫i", "√°o", "qu·∫ßn"
        ]
        
        text_lower = text.lower()
        for keyword in keywords:
            if keyword in text_lower:
                print(f"üîë Found keyword: '{keyword}'")
                for product in products:
                    if keyword in product.lower():
                        print(f"‚ö†Ô∏è Fallback match: '{product}'")
                        return product
        
        print("\n" + "="*70)
        print(f"‚ùå FAILED: Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p cho '{text}'")
        print("="*70)
        return None
        
    except requests.exceptions.Timeout:
        print("‚ùå Timeout: API kh√¥ng ph·∫£n h·ªìi trong 30 gi√¢y")
        return None
    
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Request error: {str(e)}")
        return None
    
    except Exception as e:
        print(f"‚ùå Unexpected error: {type(e).__name__} - {str(e)}")
        return None


# ==================== TEST FUNCTION ====================

if __name__ == "__main__":
    print("\n" + "üéØ"*35)
    print("GROQ LLAMA 4 SCOUT VISION - PRODUCT SEARCH TEST")
    print(f"Model: {VISION_MODEL}")
    print("üéØ"*35 + "\n")
    
    # Test 1: URL ·∫£nh
    test_url = "https://example.com/food.jpg"
    print("\n" + "="*70)
    print("TEST 1: Image from URL")
    print("="*70)
    result = groq_search_product_by_image(test_url)
    print(f"\nüìä FINAL RESULT: {result}")
    
    # Test 2: Base64 t·ª´ file local
    print("\n\n" + "="*70)
    print("TEST 2: Image from local file (uncomment to test)")
    print("="*70)
    print("""
    # ƒê·ªÉ test v·ªõi file local:
    import os
    
    image_path = "path/to/your/image.jpg"
    
    if os.path.exists(image_path):
        with open(image_path, "rb") as f:
            base64_data = base64.b64encode(f.read()).decode('utf-8')
            result = groq_search_product_by_image(base64_data)
            print(f"üìä FINAL RESULT: {result}")
    else:
        print(f"‚ö†Ô∏è File not found: {image_path}")
    """)